<!DOCTYPE html>
<html>

<head>
    
    <meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="chrome=1">
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="referrer" content="no-referrer">
<meta name="description" content="A new Hugo site.">
<title>
Key Techniques in NLP - Call me Tommy
</title>




<link rel="shortcut icon" href="/sam.ico">








<link rel="stylesheet" href="/css/main.min.8838e9b23ac21efd16a4bff8ff077840765452fb485b7a4141652c7d2bc7799b.css" integrity="sha256-iDjpsjrCHv0WpL/4/wd4QHZUUvtIW3pBQWUsfSvHeZs=" crossorigin="anonymous" media="screen">



 

<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Didact+Gothic">
<script type="text/javascript"
        async
        src="https://cdn.bootcss.com/mathjax/2.7.3/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$'], ['\[\[','\]\]']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
});

MathJax.Hub.Queue(function() {
    
    
    
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script>

<style>
code.has-jax {
    font: inherit;
    font-size: 100%;
    background: inherit;
    border: inherit;
    color: #515151;
}
</style>

<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://qhjqhj00.github.io/tn.png"/>

<meta name="twitter:title" content="Key Techniques in NLP"/>
<meta name="twitter:description" content="深度学习在自然语言处理中的关键技术  ​	本文主体部分翻译自Sebastian Ruder的推文，该推文在很多地方都有推荐。
​	原文链接：http://ruder.io/deep-learning-nlp-best-practices/index.html#attention
​	这篇推文总结了深度学习在自然处理语言中任务的实践范例，我在翻译中对内容有删减增改。平时我学习工作中每天都会阅读新公布的论文，一段时间后，发现自己收获有限，萌生了做笔记的想法（我的十八年求学生涯中从没做过笔记: ) )。
​	翻译者 / 增补者：Hongjin
1. 前述 ​	这篇推文总结了深度学习在自然处理语言中任务的实践范例。作者会定期更新本文，加入新涌现的实用技术和观点，也是我们对深度学习自然语言处理领域技术革新的追踪。
​	过去几年，在我们这个领域有个经久不衰的笑话：遇到任何任务，直接上LSTM加注意力机制，节能获得一流的效果。而且这样的境况在过去几年几乎一尘不变。
​	然而事情正在起变化，这一潭死水近来有了波澜，一些任务基准被新的模型刷新了。这也让我们这个行业一点点翻过LSTM这一页。面对这样的变化，我们应该以决绝的态度拥抱新模型，不要再拘于在LSTM的高楼上锦上添花，更不要挣扎于刷新某一个具体任务的成绩。
​	阅读这篇推文前，你被假定对神经网络在自然语言处理中的应用已经有一些了解了，并对相关任务有过实践。最重要的，是对自然语言处理这个行业有着极大的热情。
​	Tips: 不带缘由地假定什么是最好的有违公平性：依据是什么？是不是有更好的？所以，本文中提到的&quot;最佳实践&quot;一方面来自我个人的理解和经验，另一方面来自业界团队的评价。我也会尽可能为每个实践给出至少两个参考。
2. 最佳实践 1). 词向量 ​	于情于理，词向量都是近来最广为人知的实践范例了。词向量对绝大多数自然语言任务裨益良多(Kim, 2014)[^f1]。词向量的最优维度在取决于不同任务：低维度的词向量往往适用于句法分析任务，如实体识别和词性识别(Melamud et al., 2016) (Plank et al., 2016) 12。而高维度词向量则更适用于语义分析任务，例如情感分析(Ruder et al., 2016)3。
​	然而，近来随着语境化生成式词向量的兴起，句法分析任务的纪录榜单几乎都被高维度词向量模型占据了(Peters et al., 2018) (Devlin et al., 2018) (Akbik et al.,2018) (Baevski et al., 2019)4567。
2). 深度 ​	尽管我们不会抵达视觉领域的层深，自然语言处理领域在深层网络上也略有进展。深度BiLSTM网络，通常由三四层那么深，在POS标注和语义角色标注任务上也取得了领先的成绩(Plank et al., 2016) (He et al."/>

<meta property="og:title" content="Key Techniques in NLP" />
<meta property="og:description" content="深度学习在自然语言处理中的关键技术  ​	本文主体部分翻译自Sebastian Ruder的推文，该推文在很多地方都有推荐。
​	原文链接：http://ruder.io/deep-learning-nlp-best-practices/index.html#attention
​	这篇推文总结了深度学习在自然处理语言中任务的实践范例，我在翻译中对内容有删减增改。平时我学习工作中每天都会阅读新公布的论文，一段时间后，发现自己收获有限，萌生了做笔记的想法（我的十八年求学生涯中从没做过笔记: ) )。
​	翻译者 / 增补者：Hongjin
1. 前述 ​	这篇推文总结了深度学习在自然处理语言中任务的实践范例。作者会定期更新本文，加入新涌现的实用技术和观点，也是我们对深度学习自然语言处理领域技术革新的追踪。
​	过去几年，在我们这个领域有个经久不衰的笑话：遇到任何任务，直接上LSTM加注意力机制，节能获得一流的效果。而且这样的境况在过去几年几乎一尘不变。
​	然而事情正在起变化，这一潭死水近来有了波澜，一些任务基准被新的模型刷新了。这也让我们这个行业一点点翻过LSTM这一页。面对这样的变化，我们应该以决绝的态度拥抱新模型，不要再拘于在LSTM的高楼上锦上添花，更不要挣扎于刷新某一个具体任务的成绩。
​	阅读这篇推文前，你被假定对神经网络在自然语言处理中的应用已经有一些了解了，并对相关任务有过实践。最重要的，是对自然语言处理这个行业有着极大的热情。
​	Tips: 不带缘由地假定什么是最好的有违公平性：依据是什么？是不是有更好的？所以，本文中提到的&quot;最佳实践&quot;一方面来自我个人的理解和经验，另一方面来自业界团队的评价。我也会尽可能为每个实践给出至少两个参考。
2. 最佳实践 1). 词向量 ​	于情于理，词向量都是近来最广为人知的实践范例了。词向量对绝大多数自然语言任务裨益良多(Kim, 2014)[^f1]。词向量的最优维度在取决于不同任务：低维度的词向量往往适用于句法分析任务，如实体识别和词性识别(Melamud et al., 2016) (Plank et al., 2016) 12。而高维度词向量则更适用于语义分析任务，例如情感分析(Ruder et al., 2016)3。
​	然而，近来随着语境化生成式词向量的兴起，句法分析任务的纪录榜单几乎都被高维度词向量模型占据了(Peters et al., 2018) (Devlin et al., 2018) (Akbik et al.,2018) (Baevski et al., 2019)4567。
2). 深度 ​	尽管我们不会抵达视觉领域的层深，自然语言处理领域在深层网络上也略有进展。深度BiLSTM网络，通常由三四层那么深，在POS标注和语义角色标注任务上也取得了领先的成绩(Plank et al., 2016) (He et al." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://qhjqhj00.github.io/techniques/key-techniques-in-nlp/" />
<meta property="og:image" content="https://qhjqhj00.github.io/tn.png"/>
<meta property="article:published_time" content="2019-08-05T12:35:11-05:00" />
<meta property="article:modified_time" content="2019-08-05T12:35:11-05:00" /><meta property="og:site_name" content="Call me Tommy" />



    

    
    
    
    <title>
        
        Key Techniques in NLP
        
    </title>
</head>

<body>
    <div class="wrap">
        <div class="section" id="title">Key Techniques in NLP</div>

        <div class="section" id="content"><h1 id="深度学习在自然语言处理中的关键技术">深度学习在自然语言处理中的关键技术</h1>
<hr>
<p>​	本文主体部分翻译自Sebastian Ruder的推文，该推文在很多地方都有推荐。</p>
<p>​	原文链接：http://ruder.io/deep-learning-nlp-best-practices/index.html#attention</p>
<p>​	这篇推文总结了深度学习在自然处理语言中任务的实践范例，我在翻译中对内容有删减增改。平时我学习工作中每天都会阅读新公布的论文，一段时间后，发现自己收获有限，萌生了做笔记的想法（我的十八年求学生涯中从没做过笔记: ) )。</p>
<p>​	翻译者 / 增补者：Hongjin</p>
<h2 id="1-前述">1. 前述</h2>
<p>​	这篇推文总结了深度学习在自然处理语言中任务的实践范例。作者会定期更新本文，加入新涌现的实用技术和观点，也是我们对深度学习自然语言处理领域技术革新的追踪。</p>
<p>​	过去几年，在我们这个领域有个经久不衰的笑话：遇到任何任务，直接上LSTM加注意力机制，节能获得一流的效果。而且这样的境况在过去几年几乎一尘不变。</p>
<p><img src="/techniques/images/joke.jpg" alt="joke"></p>
<p>​	然而事情正在起变化，这一潭死水近来有了波澜，一些任务基准被新的模型刷新了。这也让我们这个行业一点点翻过LSTM这一页。面对这样的变化，我们应该以决绝的态度拥抱新模型，不要再拘于在LSTM的高楼上锦上添花，更不要挣扎于刷新某一个具体任务的成绩。</p>
<p>​	阅读这篇推文前，你被假定对神经网络在自然语言处理中的应用已经有一些了解了，并对相关任务有过实践。最重要的，是对自然语言处理这个行业有着极大的热情。</p>
<p>​	<strong>Tips:</strong> 不带缘由地假定什么是最好的有违公平性：依据是什么？是不是有更好的？所以，本文中提到的&quot;最佳实践&quot;一方面来自我个人的理解和经验，另一方面来自业界团队的评价。我也会尽可能为每个实践给出至少两个参考。</p>
<h2 id="2-最佳实践">2. 最佳实践</h2>
<h3 id="1-词向量">1). 词向量</h3>
<p>​	于情于理，词向量都是近来最广为人知的实践范例了。词向量对绝大多数自然语言任务裨益良多(Kim, 2014)[^f1]。词向量的最优维度在取决于不同任务：低维度的词向量往往适用于句法分析任务，如实体识别和词性识别(Melamud et al., 2016) (Plank et al., 2016) <sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup><sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>。而高维度词向量则更适用于语义分析任务，例如情感分析(Ruder et al., 2016)<sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup>。</p>
<p>​	然而，近来随着语境化生成式词向量的兴起，句法分析任务的纪录榜单几乎都被高维度词向量模型占据了(Peters et al., 2018) (Devlin et al., 2018) (Akbik et al.,2018) (Baevski et al., 2019)<sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup><sup id="fnref:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup><sup id="fnref:6"><a href="#fn:6" class="footnote-ref" role="doc-noteref">6</a></sup><sup id="fnref:7"><a href="#fn:7" class="footnote-ref" role="doc-noteref">7</a></sup>。</p>
<h3 id="2-深度">2). 深度</h3>
<p>​	尽管我们不会抵达视觉领域的层深，自然语言处理领域在深层网络上也略有进展。深度BiLSTM网络，通常由三四层那么深，在POS标注和语义角色标注任务上也取得了领先的成绩(Plank et al., 2016)  (He et al., 2017) <sup id="fnref:8"><a href="#fn:8" class="footnote-ref" role="doc-noteref">8</a></sup><sup id="fnref:9"><a href="#fn:9" class="footnote-ref" role="doc-noteref">9</a></sup>。对于特定的任务，模型会变得更深，比如谷歌的机器翻译模型就有八层encoder和decoder (Wu et al., 2016)<sup id="fnref:10"><a href="#fn:10" class="footnote-ref" role="doc-noteref">10</a></sup>。然而，对于大部分情况，两层以上的边际层深对效果的提升就会变的极其有限 (Reimers &amp; Gurevych, 2017) <sup id="fnref:11"><a href="#fn:11" class="footnote-ref" role="doc-noteref">11</a></sup>。</p>
<p>​	以上结论对大部分序列标注和结构预测任务有效，在文本分类领域情况可能不同，一些深度网络配合字符级输入或者浅层词级输入仍能取得领先的成绩(Zhang et al., 2015; Conneau et al., 2016; Le et al., 2017) [][]<sup id="fnref:12"><a href="#fn:12" class="footnote-ref" role="doc-noteref">12</a></sup><sup id="fnref:13"><a href="#fn:13" class="footnote-ref" role="doc-noteref">13</a></sup><sup id="fnref:14"><a href="#fn:14" class="footnote-ref" role="doc-noteref">14</a></sup>。</p>
<h3 id="3-层连">3). 层连</h3>
<p>​	在训练深度神经网络时，采用一些技巧可以有效地避免梯度消失问题，比如不同的层和连接方式。这里，我们讨论以下三种：i). highway layers, ii). residual connection, iii). dense connection</p>
<h4 id="highway-layers">Highway layers</h4>
<p>​	Highway layers 的出现是受到了LSTM中的门机制的启发 (Srivastava et al., 2015)<sup id="fnref:15"><a href="#fn:15" class="footnote-ref" role="doc-noteref">15</a></sup>。我们先考虑简单的一层感知器，它采用非线性函数$g$来对输入$x$进行了映射：
$$
\bf{h}=\mathcal{g}(\bf{Wx}+\bf{b})
$$
​	如果采用Highway layers，则用一下公式来计算：
$$
\bf{h}=\bf{t}\odot\mathcal{g}(\bf{Wx+\bf{b}})+(1-\bf{t})\odot\bf{x}
$$
​	$\odot$是元素级的乘。</p>
<p>​	其中，$\mathcal{g}(\bf{W_Tx+\bf{b_t}})$被称为transform门，$(1-\bf{t})$被称为carry门。从上式我们可以看出，类似LSTM的门机制，carry门适应性地让一些输入特征直接输出。</p>
<p>​	更多关于Highway layers的信息可以参考：</p>
<h4 id="residual-connection">Residual connection</h4>
<p>​	Residual connection一开始在视觉领域广为应用，它是一种比highway layers更直接的方式：
$$
\boldsymbol{h}=\mathcal{g}(\boldsymbol{Wx}+\boldsymbol{b})+\boldsymbol{x}
$$
​	它直接把当前层的输入特征加到输出特征上。这样的连接方式有效的解决了梯度消失问题，因为如果这一层梯度消失了，在这种连接方式下，这一层就类似被略过。</p>
<h4 id="dense-connection">Dense connection</h4>
<p>​	与residual connection仅仅与上一层的特征求和上不同，dense connection是把这一层之前所有层的输出都与本层输出连接(Huang et al., 2017) <sup id="fnref:16"><a href="#fn:16" class="footnote-ref" role="doc-noteref">16</a></sup>:
$$
\boldsymbol{h^\boldsymbol{l}}=\boldsymbol{g}(\boldsymbol{W[x^1;&hellip;;x^{\mathcal{l}}]+b})
$$
​	这里，$[\cdot ; \cdot]$代表concatenation。Dense connection在视觉领域取得了巨大成功。在自然语言处理领域中，这样的方式对多任务学习(Multi-Task Learning)有一定帮助 (Ruder et al., 2017)<sup id="fnref:17"><a href="#fn:17" class="footnote-ref" role="doc-noteref">17</a></sup>，在机器翻译领域，dense connection被发现比residual connection稍胜一筹(Britz et al., 2017)<sup id="fnref:18"><a href="#fn:18" class="footnote-ref" role="doc-noteref">18</a></sup>。</p>
<h3 id="4-dropout">4). Dropout</h3>
<h3 id="5-多任务学习multi-task-learning">5). 多任务学习(Multi-Task Learning)</h3>
<p>​	如果有更多与任务相关的数据，在多任务学习的帮助下往往能获得更好的效果。关于多任务学习的更多信息，可以参考http://ruder.io/multi-task/index.html。</p>
<h4 id="auxiliary-objectives-辅助目标">Auxiliary objectives (辅助目标)</h4>
<p>​	在实际任务中，通常来说都是能找到一些辅助目标函数来提升任务效果的(Ruder, 2017)<sup id="fnref:19"><a href="#fn:19" class="footnote-ref" role="doc-noteref">19</a></sup>。比如，虽然我们在训练词向量的时候已经采用了用前后文来预测当前词的方法(Mikolov et al., 2013)，我们仍然可以用这个想法来在训练模型的时候做辅助目标函数<sup id="fnref:20"><a href="#fn:20" class="footnote-ref" role="doc-noteref">20</a></sup>。同样的方法在Seq2Seq任务中同样被使用<sup id="fnref:21"><a href="#fn:21" class="footnote-ref" role="doc-noteref">21</a></sup>。</p>
<h4 id="task-specific-layers-指定目标">Task-specific layers (指定目标)</h4>
<p>​	虽然常用的多任务学习方法是直接共享参数，但让模型从单独任务的网络层(task-specific layers)中学习参数同样有效。这可以通过在为特征任务在模型的底部再接一层输出层来实现(Søgaard &amp; Goldberg, 2016)<sup id="fnref:22"><a href="#fn:22" class="footnote-ref" role="doc-noteref">22</a></sup>。另一种方式是同时引入私有的和共享的子空间 (Liu et al., 2017; Ruder et al., 2017)<sup id="fnref:23"><a href="#fn:23" class="footnote-ref" role="doc-noteref">23</a></sup><sup id="fnref:24"><a href="#fn:24" class="footnote-ref" role="doc-noteref">24</a></sup>。</p>
<h3 id="6-注意力">6). 注意力</h3>
<p>​	注意力机制在Seq2Seq模型中广受欢迎，它能注意到编码层状态（encoder states）中重要的信息。它在其他的序列模型中同样很有用，因为它可以关注到过去状态中重要的信息。注意力机制中，基于隐藏状态$s_1,s_2,…,s_m$, 我们可以获得一个带有上下文信息的向量$c_i$，这个向量是隐藏状态的加权和，权重$a_i$是可学习的参数。
$$
\mathcal{c_i}=\sum_ja_{ij}s_j
$$</p>
<p>$$
\mathcal{a_i}=softmax(f_{att}({\mathcal{h_i},s_j}))
$$</p>
<p>​	注意力函数$f_{att}(\mathcal{h_i},\mathcal{s_j})$是根据当前隐藏状态和历史隐藏状态来计算一个未标准化的注意力分值。这个函数的选择就对应了不同的attention机制，接下来我们会讨论几种注意力机制：</p>
<h4 id="additive-attention">Additive attention</h4>
<p>​	最初的attention机制用了单层的前向网络来计算attention分值<sup id="fnref:25"><a href="#fn:25" class="footnote-ref" role="doc-noteref">25</a></sup>：
$$
f_{att}(\boldsymbol{h}_i,\boldsymbol{s}_j)=\boldsymbol{v}_a^{\top}tanh(\boldsymbol{W}_a[\boldsymbol{h}_i;\boldsymbol{s}_j])
$$
​	其中，$\boldsymbol{v}_a$和$\boldsymbol{W}_a$是两个可学习的参数。类似地，我们可以分别用$\boldsymbol{W}_1$和$\boldsymbol{W}_2$来完成对$\boldsymbol{h}_i$和$s_j$的转换，然后可以把他们相加起来：
$$
f_{att}(\boldsymbol{h}_i,\boldsymbol{s}_j)=\boldsymbol{v}_a^{\top}tanh(\boldsymbol{W}_1\boldsymbol{h}_i+\boldsymbol{W}_{2}\boldsymbol{s}_j)
$$</p>
<h4 id="multiplicative-attention">Multiplicative attention</h4>
<p>​	这种注意力机制比上面那种简化了计算：
$$
f_{att}(h_i,s_j)=h_i^{\top}\boldsymbol{W}_as_j
$$</p>
<p>​	Additive attention和multiplicative attention在复杂度上相似，但是由于矩阵乘法比较高效，在实践中multiplicative attention时空更友好。当$d_h$比较小时，multiplicative attention和additive attention两种都表现优秀，但在$d_h$比较大时，addictive attention表现会跟好。一种解决这个问题的方法是做一个对$f_{att}$做一个缩放，乘$1/\sqrt{d_h}$(Vaswani et al., 2017)<sup id="fnref:26"><a href="#fn:26" class="footnote-ref" role="doc-noteref">26</a></sup>。</p>
<p>​	Attention机制不仅仅可以关注到RNN类网络中当前状态之前的状态，同样可以关注到其他特征的分布，比如说在机器翻译任务中对词向量的关注 (Vaswani et al., 2017)<sup id="fnref:27"><a href="#fn:27" class="footnote-ref" role="doc-noteref">27</a></sup>。</p>
<h4 id="self-attention">Self-attention</h4>
<p>​	即使没有其他多余的信息，我们仍可以用注意力机制来获得自身重要的信息(Lin et al., 2017) <sup id="fnref:28"><a href="#fn:28" class="footnote-ref" role="doc-noteref">28</a></sup>。Self-attention，也被称为内注意力，在包括阅读理解(Cheng et al., 2016)<sup id="fnref:29"><a href="#fn:29" class="footnote-ref" role="doc-noteref">29</a></sup>，文本蕴含 (Parikh et al., 2016)<sup id="fnref:30"><a href="#fn:30" class="footnote-ref" role="doc-noteref">30</a></sup>，摘要抽取(Paulus et al., 2017)<sup id="fnref:31"><a href="#fn:31" class="footnote-ref" role="doc-noteref">31</a></sup>等多种任务中已广受欢迎。</p>
<p>​	我们可以简单地用下面公式来计算每个隐藏状态的attention score：
$$
f_{att}(\boldsymbol{h}_i)=\boldsymbol{v}_a^{\top}tanh(\boldsymbol{W}_a\boldsymbol{h}_i)
$$
​	以矩阵形式表达的话，对于序列的隐藏状态$\boldsymbol{H}=\boldsymbol{h_1},…,\boldsymbol{h_n}$，我们可以用以下公式计算注意力分值$\boldsymbol{a}$和最终的句向量$\boldsymbol{c}$:
$$
\boldsymbol{a}=softmax(\boldsymbol{v}_atanh(\boldsymbol{W}_a\boldsymbol{H}^\top))
$$</p>
<p>$$
\boldsymbol{c}=\boldsymbol{H}\boldsymbol{a}^\top
$$</p>
<p>​	另外，我们可以通过一个$\boldsymbol{v}_a$的矩阵$\boldsymbol{V}_a$做多个self-attention来获得更多的信息：
$$
\boldsymbol{A}=softmax(\boldsymbol{V}_atanh(\boldsymbol{H}^\top))
$$</p>
<p>$$
\boldsymbol{C}=\boldsymbol{AH}
$$</p>
<p>​	实践中，为了减少冗余信息和增加注意力张量的信息多样性，可以加上一个正交约束。
$$
\Omega=|(\boldsymbol{AA}^\top-\boldsymbol{I})|^2_F
$$
​	在后来的multi-head attention 中也有类型的操作。</p>
<h4 id="key-value-attention">Key-value attention</h4>
<p>​	最后，我们介绍key-value attention，一种最近提出的注意力机制，它在多种任务中已经证明了它的有效性(Daniluk et al., 2017) (Liu &amp; Lapata, 2017)<sup id="fnref:32"><a href="#fn:32" class="footnote-ref" role="doc-noteref">32</a></sup><sup id="fnref:33"><a href="#fn:33" class="footnote-ref" role="doc-noteref">33</a></sup>。具体来说，key-value attention把隐藏状态$\boldsymbol{h}_i$分成了两部分：$\boldsymbol{h}_i=[\boldsymbol{k}_i;\boldsymbol{v}_i]$，其中keys被用来计算注意力得分：
$$
\boldsymbol{a}_i=softmax(\boldsymbol{v}<em>a^{\top}tanh(\boldsymbol{W}<em>1[\boldsymbol{k}</em>{i-L};&hellip;;\boldsymbol{k}</em>{i-1}]+(\boldsymbol{W}<em>2\boldsymbol{k}<em>i\boldsymbol{1}^\top)))
$$
​	其中，$\boldsymbol{L}$是注意力窗口的长度，$\boldsymbol{1}$是单位矩阵。这些分数接下来被用来获得句表达：$\boldsymbol{c_i}$:
$$
\boldsymbol{c_i}=[\boldsymbol{v}</em>{i-L};&hellip;;\boldsymbol{v</em>{i-1}}]\boldsymbol{a}^\top
$$</p>
<h3 id="7-优化器">7). 优化器</h3>
<p>​	如果说神经网络是黑盒，那么优化器就是黑盒中的玄学。有些时候，即使只是些微调参都能极大地改变模型的表现。例如，降低Adam中的$\beta_2$ 就可能造成很大改变(Dozat &amp; Manning, 2017) <sup id="fnref:34"><a href="#fn:34" class="footnote-ref" role="doc-noteref">34</a></sup>。</p>
<h4 id="优化算法">优化算法</h4>
<p>​	Adam是这个领域中最受欢迎和广为使用的优化算法，也是众多研究者的第一选择。Adam和SGD有很长的相爱相杀的历史。起初，初版SGD是在这个行业有着崇高的地位，而后来Adam的出现极大地遮蔽了初版SGD的荣光。一段时间后，当人们沉浸在Adam的多快好省的快乐中时，一些研究者提出，姜还是老的辣：配合衰减的学习率，SGD能比Adam强那么一丢丢  (Wu et al., 2016) <sup id="fnref:35"><a href="#fn:35" class="footnote-ref" role="doc-noteref">35</a></sup>。近来，又有学者表示，SGD带上适当的动量，同样能强过Adam (Zhang et al., 2017)<sup id="fnref:36"><a href="#fn:36" class="footnote-ref" role="doc-noteref">36</a></sup>。</p>
<h4 id="优化策略">优化策略</h4>
<p>​	尽管Adam自身就对每一个参数的学习率有调整(Ruder, 2016) <sup id="fnref:23"><a href="#fn:23" class="footnote-ref" role="doc-noteref">23</a></sup>，我们同样可以引入SGD风格的衰减策略。尤其是我们可以在重启训练的时候衰减学习率：先设定一个学习率训练模型直到收敛，然后把学习率减半，加载最好的预训练模型再训练一次。对于Adam，这样做的好处是能够把之前对参数学习率的调整忘却，重头做器。 Denkowski &amp; Neubig (2017) 的结论显示两次重启训练的Adam配合学习率衰减，比SGD配合学习率衰减更多快好省。</p>
<h2 id="heading"></h2>
<section class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1" role="doc-endnote">
<p>Melamud, O., McClosky, D., Patwardhan, S., &amp; Bansal, M. (2016). The Role of Context Types and Dimensionality in Learning Word Embeddings. In Proceedings of NAACL-HLT 2016 (pp. 1030–1040). Retrieved from <a href="http://arxiv.org/abs/1601.00893">http://arxiv.org/abs/1601.00893</a> <a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2" role="doc-endnote">
<p>Plank, B., Søgaard, A., &amp; Goldberg, Y. (2016). Multilingual Part-of-Speech Tagging with Bidirectional Long Short-Term Memory Models and Auxiliary Loss. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. <a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3" role="doc-endnote">
<p>Ruder, S., Ghaffari, P., &amp; Breslin, J. G. (2016). A Hierarchical Model of Reviews for Aspect-based Sentiment Analysis. Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP-16), 999–1005. Retrieved from <a href="http://arxiv.org/abs/1609.02745">http://arxiv.org/abs/1609.02745</a> <a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4" role="doc-endnote">
<p>Peters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., &amp; Zettlemoyer, L. (2018). Deep contextualized word representations. <em>arXiv preprint arXiv:1802.05365</em>. <a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:5" role="doc-endnote">
<p>Akbik, A., Blythe, D., &amp; Vollgraf, R. (2018, August). Contextual string embeddings for sequence labeling. In <em>Proceedings of the 27th International Conference on Computational Linguistics</em> (pp. 1638-1649). <a href="#fnref:5" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:6" role="doc-endnote">
<p>Devlin, J., Chang, M. W., Lee, K., &amp; Toutanova, K. (2018). Bert: Pre-training of deep bidirectional transformers for language understanding. <em>arXiv preprint arXiv:1810.04805</em>. <a href="#fnref:6" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:7" role="doc-endnote">
<p>Baevski, A., Edunov, S., Liu, Y., Zettlemoyer, L., &amp; Auli, M. (2019). Cloze-driven Pretraining of Self-attention Networks. <em>arXiv preprint arXiv:1903.07785</em>. <a href="#fnref:7" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:8" role="doc-endnote">
<p>Plank, B., Søgaard, A., &amp; Goldberg, Y. (2016). Multilingual part-of-speech tagging with bidirectional long short-term memory models and auxiliary loss. <em>arXiv preprint arXiv:1604.05529</em>. <a href="#fnref:8" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:9" role="doc-endnote">
<p>He, L., Lee, K., Lewis, M., &amp; Zettlemoyer, L. (2017). Deep Semantic Role Labeling: What Works and What’s Next. ACL. <a href="#fnref:9" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:10" role="doc-endnote">
<p>Wu, Y., Schuster, M., Chen, Z., Le, Q. V, Norouzi, M., Macherey, W., … Dean, J. (2016). Google’s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation. arXiv Preprint arXiv:1609.08144. <a href="#fnref:10" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:11" role="doc-endnote">
<p>Reimers, N., &amp; Gurevych, I. (2017). Optimal Hyperparameters for Deep LSTM-Networks for Sequence Labeling Tasks. In arXiv preprint arXiv:1707.06799: Retrieved from <a href="https://arxiv.org/pdf/1707.06799.pdf">https://arxiv.org/pdf/1707.06799.pdf</a> <a href="#fnref:11" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:12" role="doc-endnote">
<p>Zhang, X., Zhao, J., &amp; LeCun, Y. (2015). Character-level Convolutional Networks for Text Classification. Advances in Neural Information Processing Systems, 649–657. Retrieved from <a href="http://arxiv.org/abs/1509.01626">http://arxiv.org/abs/1509.01626</a> <a href="#fnref:12" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:13" role="doc-endnote">
<p>Conneau, A., Schwenk, H., Barrault, L., &amp; Lecun, Y. (2016). Very Deep Convolutional Networks for Natural Language Processing. arXiv Preprint arXiv:1606.01781. Retrieved from <a href="http://arxiv.org/abs/1606.01781">http://arxiv.org/abs/1606.01781</a> <a href="#fnref:13" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:14" role="doc-endnote">
<p>Le, H. T., Cerisara, C., &amp; Denis, A. (2017). Do Convolutional Networks need to be Deep for Text Classification ? In arXiv preprint arXiv:1707.04108. <a href="#fnref:14" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:15" role="doc-endnote">
<p>Huang, G., Weinberger, K. Q., &amp; Maaten, L. Van Der. (2016). Densely Connected Convolutional Networks. CVPR 2017. <a href="#fnref:15" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:16" role="doc-endnote">
<p>He, K., Zhang, X., Ren, S., &amp; Sun, J. (2016). Deep Residual Learning for Image Recognition. In CVPR. <a href="#fnref:16" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:17" role="doc-endnote">
<p>Ruder, S., Bingel, J., Augenstein, I., &amp; Søgaard, A. (2017). Sluice networks: Learning what to share between loosely related tasks. arXiv Preprint arXiv:1705.08142. Retrieved from <a href="http://arxiv.org/abs/1705.08142">http://arxiv.org/abs/1705.08142</a> <a href="#fnref:17" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:18" role="doc-endnote">
<p>Britz, D., Goldie, A., Luong, T., &amp; Le, Q. (2017). Massive Exploration of Neural Machine Translation Architectures. In arXiv preprint arXiv:1703.03906. <a href="#fnref:18" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:19" role="doc-endnote">
<p>Ruder, S. (2017). An Overview of Multi-Task Learning in Deep Neural Networks. In arXiv preprint arXiv:1706.05098. <a href="#fnref:19" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:20" role="doc-endnote">
<p>Rei, M. (2017). Semi-supervised Multitask Learning for Sequence Labeling. In Proceedings of ACL 2017. <a href="#fnref:20" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:21" role="doc-endnote">
<p>Ramachandran, P., Liu, P. J., &amp; Le, Q. V. (2016). Unsupervised Pretrainig for Sequence to Sequence Learning. arXiv Preprint arXiv:1611.02683. <a href="#fnref:21" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:22" role="doc-endnote">
<p>Søgaard, A., &amp; Goldberg, Y. (2016). Deep multi-task learning with low level tasks supervised at lower layers. Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, 231–235. <a href="#fnref:22" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:23" role="doc-endnote">
<p>Ruder, S. (2016). An overview of gradient descent optimization. arXiv Preprint arXiv:1609.04747. <a href="#fnref:23" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:24" role="doc-endnote">
<p>Liu, P., Qiu, X., &amp; Huang, X. (2017). Adversarial Multi-task Learning for Text Classification. In ACL 2017. Retrieved from <a href="http://arxiv.org/abs/1704.05742">http://arxiv.org/abs/1704.05742</a> <a href="http://ruder.io/deep-learning-nlp-best-practices/index.html#fnref30">↩︎</a> <a href="#fnref:24" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:25" role="doc-endnote">
<p>Bahdanau, D., Cho, K., &amp; Bengio, Y.. Neural Machine Translation by Jointly Learning to Align and Translate. ICLR 2015. <a href="https://doi.org/10.1146/annurev.neuro.26.041002.131047">https://doi.org/10.1146/annurev.neuro.26.041002.131047</a> <a href="#fnref:25" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:26" role="doc-endnote">
<p>Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., … Polosukhin, I. (2017). Attention Is All You Need. arXiv Preprint arXiv:1706.03762. <a href="#fnref:26" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:27" role="doc-endnote">
<p>Kadlec, R., Schmid, M., Bajgar, O., &amp; Kleindienst, J. (2016). Text Understanding with the Attention Sum Reader Network. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. <a href="#fnref:27" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:28" role="doc-endnote">
<p>Lin, Z., Feng, M., Santos, C. N. dos, Yu, M., Xiang, B., Zhou, B., &amp; Bengio, Y. (2017). A Structured Self-Attentive Sentence Embedding. In ICLR 2017. <a href="#fnref:28" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:29" role="doc-endnote">
<p>Cheng, J., Dong, L., &amp; Lapata, M. (2016). Long Short-Term Memory-Networks for Machine Reading. arXiv Preprint arXiv:1601.06733. Retrieved from <a href="http://arxiv.org/abs/1601.06733">http://arxiv.org/abs/1601.06733</a> <a href="#fnref:29" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:30" role="doc-endnote">
<p>Parikh, A. P., Täckström, O., Das, D., &amp; Uszkoreit, J. (2016). A Decomposable Attention Model for Natural Language Inference. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. Retrieved from <a href="http://arxiv.org/abs/1606.01933">http://arxiv.org/abs/1606.01933</a> <a href="#fnref:30" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:31" role="doc-endnote">
<p>Paulus, R., Xiong, C., &amp; Socher, R. (2017). A Deep Reinforced Model for Abstractive Summarization. In arXiv preprint arXiv:1705.04304,. Retrieved from <a href="http://arxiv.org/abs/1705.04304">http://arxiv.org/abs/1705.04304</a> <a href="#fnref:31" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:32" role="doc-endnote">
<p>Daniluk, M., Rockt, T., Welbl, J., &amp; Riedel, S. (2017). Frustratingly Short Attention Spans in Neural Language Modeling. In ICLR 2017. <a href="#fnref:32" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:33" role="doc-endnote">
<p>Liu, Y., &amp; Lapata, M. (2017). Learning Structured Text Representations. In arXiv preprint arXiv:1705.09207. Retrieved from <a href="http://arxiv.org/abs/1705.09207">http://arxiv.org/abs/1705.09207</a> <a href="#fnref:33" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:34" role="doc-endnote">
<p>Dozat, T., &amp; Manning, C. D. (2017). Deep Biaffine Attention for Neural Dependency Parsing. In ICLR 2017. Retrieved from <a href="http://arxiv.org/abs/1611.01734">http://arxiv.org/abs/1611.01734</a> <a href="#fnref:34" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:35" role="doc-endnote">
<p>Kingma, D. P., &amp; Ba, J. L. (2015). Adam: a Method for Stochastic Optimization. International Conference on Learning Representations. <a href="#fnref:35" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:36" role="doc-endnote">
<p>Zhang, J., Mitliagkas, I., &amp; Ré, C. (2017). YellowFin and the Art of Momentum Tuning. arXiv preprint arXiv:1706.03471. <a href="#fnref:36" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</section>
</div>

        
        <div class="section bottom-menu">
<hr />
<p>


    

    
        
            <a href="/travel">he&#39;s a traveller.</a>
        
    
    
        
            &#183; 
            <a href="/techniques">he&#39;s a coder.</a>
        
            &#183; 
            <a href="/mis">he can cook.</a>
        
            &#183; 
            <a href="/music">he plays the guitar.</a>
        
            &#183; 
            <a href="/about">who is he?</a>
        
    
    &#183; 
    <a href="https://qhjqhj00.github.io/">
        main
    </a>

</p></div>
        

        <div class="section footer">TOMMY CHIEN</div>
    </div>
</body>

</html>